# RNN 循环神经网络(Recurrent Neural Networks)



## 何时使用RNN/RNN与CNN,NN的不同

> **序列**。基于知识背景，你可能会思考：*是什么让RNN如此独特呢？*普通神经网络和卷积神经网络的一个显而易见的局限就是他们的API都过于限制：他们接收一个固定尺寸的向量作为输入（比如一张图像），并且产生一个固定尺寸的向量作为输出（比如针对不同分类的概率）。不仅如此，这些模型甚至对于上述映射的演算操作的步骤也是固定的（比如模型中的层数）。RNN之所以如此让人兴奋，其核心原因在于其允许我们对向量的序列进行操作：输入可以是序列，输出也可以是序列，在最一般化的情况下输入输出都可以是序列。



Rnn 输入与输出可以是一系列tensor，而其他cnn/nn只能是一个tensor

**用于输入输出不确定长度的时候**、与顺序关联很紧密的场景

eg.语言描述，不确定输出的句子长度



> 即使数据不是序列的形式，仍然可以构建并训练出能够进行序列化处理数据的强大模型。换句话说，你是要让模型学习到一个处理固定尺寸数据的分阶段程序。



## 基本类型

如图

![img](https://pic2.zhimg.com/80/2a37bd4e9b12bcc19e045eaf22fea4e5_720w.jpg)

上图中每个正方形代表一个向量，箭头代表函数（比如矩阵乘法）。输入向量是红色，输出向量是蓝色，绿色向量装的是RNN的状态（马上具体介绍）。从左至右为：

1. 非RNN的普通过程，从固定尺寸的输入到固定尺寸的输出（比如图像分类）。
2. 输出是序列（例如图像标注：输入是一张图像，输出是单词的序列）。
3. 输入是序列（例如情绪分析：输入是一个句子，输出是对句子属于正面还是负面情绪的分类）。
4. 输入输出都是序列（比如机器翻译：RNN输入一个英文句子输出一个法文句子）。
5. 同步的输入输出序列（比如视频分类中，我们将对视频的每一帧都打标签）。



## RNN内部结构

> 例子：你有个室友，每天给你做饭，它会做6种菜，分别为记为第1-6号菜，它做饭有个规则：
>
> 1、如果是晴天，昨天做的1号菜，今天就做2号；昨天2号菜，今天做3号菜；昨天3号菜；今天做1号菜
>
> 2、如果是雨天，昨天做的1号菜，今天就做3号；昨天2号菜，今天做1号菜；昨天3号菜；今天做2号菜
>
> 已知下一周的天气和昨天做的菜，请写一个神经网络来预测它下一周每天做的什么菜
>
> 这里输入是下一周的天气和昨天做的菜，输出是接下来7天的菜
>
> 假如使用全连接神经网路预测接下来一周做的菜，就如输入一个长度为 7\*2+3=17 的向量，输入7天的天气和前一天做的菜，再输出一个长度 7\*3=21 的向量表示预测的接下来7天的菜，经训练就会发现它并不能很好地完成任务，因为它并不能发现它的每个输出之间有关联的
>
> 我们知道，它做菜的种类与 **今天天气** 和 **昨天做的什么菜** 有关
>
> 而 昨天做的什么菜 具有顺序关系，与输出有关联，所以要将 预测得到的今天做的啥菜 作为输入 去预测明天做啥菜。这里



RNN的每次输入包含上次输出的一部分，这样它知道之前输出过啥，就能形成关联

RNNHidden state



**RNN的计算**。那么RNN到底是如何工作的呢？在其核心，RNN有一个貌似简单的API：它接收输入向量**x**，返回输出向量**y**。然而这个输出向量的内容不仅被输入数据影响，而且会收到整个历史输入的影响。写成一个类的话，RNN的API只包含了一个**step**方法：

```python
# Many to many RNN 输入每个step
rnn = RNN()

# 初始时hidden_state是空的
hidden_state = None

# 循环输入一个序列(timesteps)中的每一个向量(step)
for step in timesteps:
    y, hidden_state = rnn.step(step, hidden_state)
    # 将新输入和上次的hidden_state传回
    print y # 这一时刻的输出
```

每当**step**方法被调用的时候，RNN的内部状态就被更新。在最简单情况下，该内部装着仅包含一个内部隐向量h。下面是一个普通RNN的step方法的实现：

```python
class RNN:
  # ...
  def step(self, step, hidden_state): # x为在目前这个timestep的输入
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y
```

上面的代码详细说明了普通RNN的前向传播。该RNN的参数是三个矩阵：**W_hh, W_xh, W_hy**。隐藏状态**self.h**被初始化为零向量。**np.tanh**函数是一个非线性函数，将激活数据挤压到[-1,1]之内。注意代码是如何工作的：在tanh内有两个部分。一个是基于前一个隐藏状态，另一个是基于当前的输入。在numpy中，**np.dot**是进行矩阵乘法。两个中间变量相加，其结果被tanh处理为一个新的状态向量。如果你更喜欢用数学公式理解，那么公式是这样的：![[公式]](https://www.zhihu.com/equation?tex=h_t%3Dtanh%28W_%7Bhh%7Dh_%7Bt-1%7D%2BW_%7Bhx%7Dx_t%29)。其中tanh是逐元素进行操作的。







![img](https://pic1.zhimg.com/80/20c6a56f097aba3de796ac62c59605bc_720w.jpg)

